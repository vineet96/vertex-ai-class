{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineet96/vertex-ai-class/blob/main/Covid_Detection_Image_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwcWRPtUgUpu"
      },
      "source": [
        "\n",
        "\n",
        "**Installation**\n",
        "Install the latest version of Vertex AI SDK for Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5m8X_e3SAz7j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiOND_IegjQI"
      },
      "source": [
        "**Only for Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTme14pa9lQd"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56_ss78QiiJT"
      },
      "source": [
        "**Setting up the GCP environment variable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh8OMMkpgqIa"
      },
      "source": [
        "**Setup Project ID**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-mYxTeqBBMu"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK6uB3KxgwGw"
      },
      "source": [
        "**Setup GCP Region**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YLZQEOkgBD-k"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGMCAUz1hIXV"
      },
      "source": [
        "**Authenticate your Google Cloud account**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVY1YklZf3CU"
      },
      "source": [
        " 1. Local JupyterLab instance, uncomment and run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VmQYHU4f9Jo"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT8DTv0Of-lZ"
      },
      "source": [
        "2. For Colab, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "THt99PLXBHTI"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnX4uLh1hvs7"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V6lUAODdByfH"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi7y-kBLhlNw"
      },
      "source": [
        "## Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S1VRF7yZB8sd"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So far we have coded scafloding for using Vertex Python SDK.. Its time for actual model training**"
      ],
      "metadata": {
        "id": "TFDJ1OVXxJfI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-6pVvLvh9RG"
      },
      "source": [
        "**Create the Dataset** dataset = aiplatform.ImageDataset('dataset-id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4oMuSEiICJyH"
      },
      "outputs": [],
      "source": [
        "dataset = aiplatform.ImageDataset('your-detaset-id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns04svuViPGy"
      },
      "source": [
        "### Create and run training pipeline\n",
        "\n",
        "To train an AutoML model, you perform two steps: 1) create a training pipeline, and 2) run the pipeline.\n",
        "\n",
        "#### Create training pipeline\n",
        "\n",
        "An AutoML training pipeline is created with the `AutoMLImageTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `TrainingJob` resource.\n",
        "- `prediction_type`: The type task to train the model for.\n",
        "  - `classification`: An image classification model.\n",
        "  - `object_detection`: An image object detection model.\n",
        "- `multi_label`: If a classification task, whether single (`False`) or multi-labeled (`True`).\n",
        "- `model_type`: The type of model for deployment.\n",
        "  - `CLOUD`: Deployment on Google Cloud\n",
        "  - `CLOUD_HIGH_ACCURACY_1`: Optimized for accuracy over latency for deployment on Google Cloud.\n",
        "  - `CLOUD_LOW_LATENCY_`: Optimized for latency over accuracy for deployment on Google Cloud.\n",
        "  - `MOBILE_TF_VERSATILE_1`: Deployment on an edge device.\n",
        "  - `MOBILE_TF_HIGH_ACCURACY_1`:Optimized for accuracy over latency for deployment on an edge device.\n",
        "  - `MOBILE_TF_LOW_LATENCY_1`: Optimized for latency over accuracy for deployment on an edge device.\n",
        "- `base_model`: (optional) Transfer learning from existing `Model` resource -- supported for image classification only.\n",
        "\n",
        "The instantiated object is the DAG (directed acyclic graph) for the training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfsYsKoOCRsW"
      },
      "outputs": [],
      "source": [
        "dag = aiplatform.AutoMLImageTrainingJob(\n",
        "    display_name=\"covidmodel\",\n",
        "    prediction_type=\"classification\",\n",
        "    multi_label=False,\n",
        "    model_type=\"CLOUD\",\n",
        "    base_model=None,\n",
        ")\n",
        "\n",
        "print(dag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-LI20RpiYCV"
      },
      "source": [
        "#### Run the training pipeline\n",
        "\n",
        "Next, you run the DAG to start the training job by invoking the method `run`, with the following parameters:\n",
        "\n",
        "- `dataset`: The `Dataset` resource to train the model.\n",
        "- `model_display_name`: The human readable name for the trained model.\n",
        "- `training_fraction_split`: The percentage of the dataset to use for training.\n",
        "- `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
        "- `validation_fraction_split`: The percentage of the dataset to use for validation.\n",
        "- `budget_milli_node_hours`: (optional) Maximum training time specified in unit of millihours (1000 = hour).\n",
        "- `disable_early_stopping`: If `True`, training maybe completed before using the entire budget if the service believes it cannot further improve on the model objective measurements.\n",
        "\n",
        "The `run` method when completed returns the `Model` resource.\n",
        "\n",
        "The execution of the training pipeline will take upto 20 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbSBH4D_CXIc"
      },
      "outputs": [],
      "source": [
        "model = dag.run(\n",
        "    dataset=dataset,\n",
        "    model_display_name=\"covidmodel\",\n",
        "    training_fraction_split=0.8,\n",
        "    validation_fraction_split=0.1,\n",
        "    test_fraction_split=0.1,\n",
        "    budget_milli_node_hours=8000,\n",
        "    disable_early_stopping=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deploy the model**\n",
        "To deploy the model, you invoke the deploy method."
      ],
      "metadata": {
        "id": "YHzCC1o5sMJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = model.deploy()"
      ],
      "metadata": {
        "id": "gqBihXD2sCgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Online Prediction**"
      ],
      "metadata": {
        "id": "9_-g1oHc1HZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Image/X-Ray** "
      ],
      "metadata": {
        "id": "-tOdf82-1VqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_xray1 = 'gs://your bucket/Covid_110.png'"
      ],
      "metadata": {
        "id": "tYlCCRB_v_LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Image/X-Ray**"
      ],
      "metadata": {
        "id": "IT2kDddk1OiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_xray2 = 'gs://your bucket/Covid_128.png'"
      ],
      "metadata": {
        "id": "MRvgVMrqwDEJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing tensorflow funtion to read the content of the image stored in Cloud Storage Bucket.**"
      ],
      "metadata": {
        "id": "J_7QiEqZ1g2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "with tf.io.gfile.GFile(test_xray1, \"rb\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# The format of each instance should conform to the deployed model's prediction input schema.\n",
        "instances = [{\"content\": base64.b64encode(content).decode(\"utf-8\")}]"
      ],
      "metadata": {
        "id": "ZbvJ636AwDOG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = endpoint.predict(instances=instances)\n",
        "\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "PZOqvHGWwDU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "with tf.io.gfile.GFile(test_xray2, \"rb\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# The format of each instance should conform to the deployed model's prediction input schema.\n",
        "instances = [{\"content\": base64.b64encode(content).decode(\"utf-8\")}]\n",
        "\n",
        "prediction = endpoint.predict(instances=instances)\n",
        "\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "0AIsm3ZcwDki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Prediction**"
      ],
      "metadata": {
        "id": "q6V28YCB4RjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make the batch input file**\n",
        "Now make a batch input file, which you will store in your local Cloud Storage bucket. The batch input file can be either CSV or JSONL. You will use JSONL in this tutorial. For JSONL file, you make one dictionary entry per line for each data item (instance). The dictionary contains the key/value pairs:\n",
        "\n",
        "content: The Cloud Storage path to the image.\n",
        "mime_type: The content type. In our example, it is a png file.\n",
        "For example:\n",
        "\n",
        "      {'content': '[your-bucket]/file1.jpg', 'mime_type': 'png'}"
      ],
      "metadata": {
        "id": "QNfGVH_Z3q_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_URI = 'gs://your bucket'"
      ],
      "metadata": {
        "id": "Rhr9OuWjwE5y"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_xray1 = 'gs://your bucket/Covid_16.png'\n",
        "test_xray2 = 'gs://your bucket/Covid_17.png'\n",
        "test_xray3 = 'gs://your bucket/Covid_18.png'\n",
        "test_xray4 = 'gs://your bucket/Normal_15.jpeg'\n",
        "test_xray5 = 'gs://your bucket/Normal_16.jpeg'"
      ],
      "metadata": {
        "id": "uVD6eSDkwFEe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "gcs_input_uri = BUCKET_URI + \"/test.jsonl\"\n",
        "with tf.io.gfile.GFile(gcs_input_uri, \"w\") as f:\n",
        "    data = {\"content\": test_xray1, \"mime_type\": \"image/png\"}\n",
        "    f.write(json.dumps(data) + \"\\n\")\n",
        "    data = {\"content\": test_xray2, \"mime_type\": \"image/png\"}\n",
        "    f.write(json.dumps(data) + \"\\n\")\n",
        "    data = {\"content\": test_xray3, \"mime_type\": \"image/png\"}\n",
        "    f.write(json.dumps(data) + \"\\n\")\n",
        "    data = {\"content\": test_xray4, \"mime_type\": \"image/jpeg\"}\n",
        "    f.write(json.dumps(data) + \"\\n\")\n",
        "    data = {\"content\": test_xray5, \"mime_type\": \"image/jpeg\"}\n",
        "    f.write(json.dumps(data) + \"\\n\")\n",
        "    \n",
        "\n",
        "print(gcs_input_uri)\n",
        "! gsutil cat $gcs_input_uri"
      ],
      "metadata": {
        "id": "e--mzmg_v_Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make the batch prediction request\n",
        "\n",
        "Now that your Model resource is trained, you can make a batch prediction by invoking the batch_predict() method, with the following parameters:\n",
        "\n",
        "- `job_display_name`: The human readable name for the batch prediction job.\n",
        "- `gcs_source`: A list of one or more batch request input files.\n",
        "- `gcs_destination_prefix`: The Cloud Storage location for storing the batch prediction resuls.\n",
        "- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y6_xAfz471Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_predict_job = model.batch_predict(\n",
        "    job_display_name=\"covidprediction\",\n",
        "    gcs_source=gcs_input_uri,\n",
        "    gcs_destination_prefix=BUCKET_URI,\n",
        "    sync=False,\n",
        ")\n",
        "\n",
        "print(batch_predict_job)"
      ],
      "metadata": {
        "id": "09voq0Wk7RKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_predict_job.wait()"
      ],
      "metadata": {
        "id": "z1jaYEYzv_Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial."
      ],
      "metadata": {
        "id": "9hN3PtP0Aqac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Delete the dataset using the Vertex dataset object\n",
        "#dataset.delete()\n",
        "\n",
        "endpoint.undeploy_all()\n",
        "\n",
        "endpoint.delete()\n",
        "\n",
        "# Delete the model using the Vertex model object\n",
        "model.delete()\n",
        "\n",
        "# Delete the AutoML trainig job\n",
        "dag.delete()\n",
        "\n",
        "# Delete the batch prediction job\n",
        "batch_predict_job.delete()\n",
        "\n"
      ],
      "metadata": {
        "id": "mdF-AdOJAtZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeJPoj4EQc0p26r1AfGn/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}